{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path_to_data):\n",
    "    with open(path_to_data) as f:\n",
    "        data = []\n",
    "\n",
    "        col_names = None\n",
    "\n",
    "        for row in csv.reader(f):\n",
    "            if not col_names:\n",
    "                col_names = row\n",
    "                col_names[0] = 'age'\n",
    "            else:\n",
    "                data.append([float(i) for i in row])\n",
    "\n",
    "    return col_names, data\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    Use z-score of input feature values\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html\n",
    "    https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)\n",
    "    \"\"\"\n",
    "    feature_vectors = []\n",
    "    output_classes = []\n",
    "    \n",
    "    for entry in data:\n",
    "        feature_vectors.append(entry[0:-1])\n",
    "        output_classes.append([entry[-1]])\n",
    "        \n",
    "    feature_vectors = np.array(feature_vectors).astype(np.float64)\n",
    "    feature_vectors = np.apply_along_axis(stats.zscore, 0, feature_vectors)\n",
    "        \n",
    "    return feature_vectors, np.array(output_classes), np.hstack((feature_vectors, np.array(output_classes)))\n",
    "\n",
    "def split_data(all_data, train_perc=0.85, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    num_train = math.floor(train_perc * len(all_data))\n",
    "    \n",
    "    use_data = copy.deepcopy(all_data)\n",
    "    \n",
    "    np.random.shuffle(use_data)\n",
    "    \n",
    "    training = []\n",
    "    while len(training) < num_train:\n",
    "        sample, use_data = use_data[-1], use_data[:-1]\n",
    "        \n",
    "        training.append(sample)\n",
    "    testing = use_data\n",
    "    \n",
    "    training_xs = []\n",
    "    training_ys = []\n",
    "    for ind in range(len(training)):\n",
    "        training_xs.append(training[ind][0:-1])\n",
    "        training_ys.append(training[ind][-1])\n",
    "    \n",
    "    testing_xs = []\n",
    "    testing_ys = []\n",
    "    for ind in range(len(testing)):\n",
    "        testing_xs.append(testing[ind][0:-1])\n",
    "        testing_ys.append(testing[ind][-1])\n",
    "        \n",
    "    training_ys = np.expand_dims(training_ys, 1)\n",
    "    testing_ys = np.expand_dims(testing_ys, 1)\n",
    "        \n",
    "    return ((np.array(training_xs), np.array(training_ys).astype(int)), (np.array(testing_xs), np.array(testing_ys).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(func):\n",
    "    return 1 / (1 + np.exp(-func))\n",
    "\n",
    "def calc_gradient((xs, ys), w):\n",
    "    raw_scores = np.dot(xs, w)\n",
    "    adjusted_predictions = sigmoid(raw_scores)\n",
    "    errors = ys - adjusted_predictions\n",
    "    gradient = -np.dot(xs.transpose(), errors)\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent((orig_xs, ys), epochs=100000, lr=1e-6, prec=1e-15):\n",
    "    intercept = np.ones((orig_xs.shape[0], 1))\n",
    "    xs = np.hstack((intercept, orig_xs))\n",
    "        \n",
    "    w = np.zeros(xs.shape[1])\n",
    "    w = np.expand_dims(w, 1)\n",
    "    w = w.astype(np.float128)\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "        gradient = calc_gradient((xs, ys), w)\n",
    "        update = lr * gradient\n",
    "        w = w - update\n",
    "        if (update < prec).all():\n",
    "            return w\n",
    "        \n",
    "    return w\n",
    "\n",
    "def eval_set((set_xs, set_ys), w):\n",
    "    with_intercept = np.ones((set_xs.shape[0], 1))\n",
    "    use_xs = np.hstack((with_intercept, set_xs))\n",
    "    raw_scores = np.dot(use_xs, w)\n",
    "    predictions = np.round(sigmoid(raw_scores)).astype(int)\n",
    "    error = (predictions != set_ys).sum().astype(float) / len(set_ys)\n",
    "    accuracy = np.mean(predictions == set_ys)\n",
    "    return error, accuracy\n",
    "\n",
    "def split_into_k((xs, ys), k):\n",
    "    xs_copy = list(copy.deepcopy(xs))\n",
    "    ys_copy = list(copy.deepcopy(ys))\n",
    "    \n",
    "    xs_ys_pairs = []\n",
    "\n",
    "    fold_len = int(len(xs) / k)\n",
    "    \n",
    "    for _ in range(k):\n",
    "        this_fold_xs = []\n",
    "        this_fold_ys = []\n",
    "        \n",
    "        while len(this_fold_xs) < fold_len:\n",
    "            ind = randrange(len(xs_copy))\n",
    "            this_fold_xs.append(xs_copy.pop(ind))\n",
    "            this_fold_ys.append(ys_copy.pop(ind))\n",
    "        \n",
    "        xs_ys_pairs.append((np.array(this_fold_xs), np.array(this_fold_ys)))\n",
    "        \n",
    "    return xs_ys_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 training samples\n",
      "31 testing samples\n"
     ]
    }
   ],
   "source": [
    "names, data_points = load_csv('../data/orig_raw.csv')\n",
    "all_xs, all_ys, adjusted_data = prepare_data(data_points)\n",
    "training, testing = split_data(adjusted_data, train_perc=0.90)\n",
    "print('{} training samples'.format(len(training[0])))\n",
    "print('{} testing samples'.format(len(testing[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error=0.165441176471 train_accuracy=0.834558823529\n",
      "test_error=0.161290322581 test_accuracy=0.838709677419\n"
     ]
    }
   ],
   "source": [
    "weights = gradient_descent(training, epochs=250000, lr=0.0001)\n",
    "train_error, train_accuracy = eval_set(training, weights)\n",
    "test_error, test_accuracy = eval_set(testing, weights)\n",
    "print('train_error={} train_accuracy={}'.format(train_error, train_accuracy))\n",
    "print('test_error={} test_accuracy={}'.format(test_error, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2 error=0.191176470588\n",
      "k=5 error=0.148148148148\n",
      "k=10 error=0.0740740740741\n",
      "lowest_error=0.0740740740741\n",
      "best_weights=[[ 0.12154886]\n",
      " [-0.21601028]\n",
      " [-0.78352644]\n",
      " [ 0.91734276]\n",
      " [-0.24693398]\n",
      " [-0.15110997]\n",
      " [-0.15597072]\n",
      " [ 0.11015886]\n",
      " [ 0.34344169]\n",
      " [-0.42677931]\n",
      " [-0.60262549]\n",
      " [ 0.37276644]\n",
      " [-0.88942744]\n",
      " [-0.57823551]]\n"
     ]
    }
   ],
   "source": [
    "ks = [2, 5, 10]\n",
    "k_errors = []\n",
    "k_weights = []\n",
    "\n",
    "for k in ks:\n",
    "    all_training_xs_ys_pairs = split_into_k(training, k)\n",
    "    holdout_errors = []\n",
    "    holdout_weights = []\n",
    "    \n",
    "    for holdout_ind in range(len(all_training_xs_ys_pairs)):\n",
    "        training_copy = copy.deepcopy(all_training_xs_ys_pairs)\n",
    "        holdout_xs, holdout_ys = training_copy.pop(holdout_ind)\n",
    "        xs = training_copy[0][0]\n",
    "        ys = training_copy[0][1]\n",
    "        \n",
    "        for ind in range(1, len(training_copy)):\n",
    "            a_fold_xs, a_fold_ys = training_copy[ind]\n",
    "            xs = np.concatenate((xs, a_fold_xs), axis=0)\n",
    "            ys = np.concatenate((ys, a_fold_ys), axis=0)\n",
    "            \n",
    "        weights = gradient_descent((xs, ys), epochs=100000, lr=0.0001)\n",
    "        error, accuracy = eval_set((holdout_xs, holdout_ys), weights)\n",
    "\n",
    "        holdout_errors.append(error)\n",
    "        holdout_weights.append(weights)\n",
    "        \n",
    "    min_error_ind = holdout_errors.index(min(holdout_errors))\n",
    "        \n",
    "    holdout_error = holdout_errors[min_error_ind]\n",
    "    k_errors.append(holdout_error)\n",
    "    best_weight = holdout_weights[min_error_ind]\n",
    "    k_weights.append(best_weight)\n",
    "    \n",
    "    print('k={} error={}').format(k, holdout_error)\n",
    "    \n",
    "lowest_error_ind = k_errors.index(min(k_errors))\n",
    "lowest_error = k_errors[lowest_error_ind]\n",
    "best_weights = k_weights[lowest_error_ind]\n",
    "\n",
    "print('lowest_error={}'.format(lowest_error))\n",
    "print('best_weights={}'.format(best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_error=0.161716171617 all_accuracy=0.838283828383\n"
     ]
    }
   ],
   "source": [
    "all_error, all_accuracy = eval_set((all_xs, all_ys), best_weights)\n",
    "print('all_error={} all_accuracy={}'.format(all_error, all_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
